{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest01\\AppData\\Local\\Temp\\ipykernel_15688\\2973047249.py:38: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df.groupby(['Property_Type', 'Town/City'], group_keys=False).apply(lambda x: x.sample(frac=0.75, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to downcast numeric columns to save memory\n",
    "def downcast(df):\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "columns_to_use = [\n",
    "    '70000', 'D', 'N', 'F', 'MK15 9HP', 'WILLEN', 'MILTON KEYNES', \n",
    "    'MILTON KEYNES.1', 'MILTON KEYNES.2', '1995-07-07 00:00', 'A'\n",
    "]\n",
    "\n",
    "df = pd.read_csv('C:/Users/Guest01/Documents/Manpreet_thesis/Datasets/UK_property_price/UKarchive/202304.csv', usecols=columns_to_use)\n",
    "\n",
    "# Rename the columns for ease of use\n",
    "df = df.rename(columns={\n",
    "    '70000': 'price', \n",
    "    'D': 'Property_Type', \n",
    "    'N': 'Old/New', \n",
    "    'F': 'Duration', \n",
    "    'MK15 9HP': 'Postcode', \n",
    "    'WILLEN': 'Locality', \n",
    "    'MILTON KEYNES': 'Town/City', \n",
    "    'MILTON KEYNES.1': 'District', \n",
    "    'MILTON KEYNES.2': 'County', \n",
    "    '1995-07-07 00:00': 'Date_of_Transfer', \n",
    "    'A': 'PPDCategory_Type'\n",
    "})\n",
    "\n",
    "# Apply downcasting to reduce memory usage immediately\n",
    "df = downcast(df)\n",
    "\n",
    "# Stratified sampling to reduce dataset size (75% sampling by 'Property_Type' and 'Town/City')\n",
    "df_sampled = df.groupby(['Property_Type', 'Town/City'], group_keys=False).apply(lambda x: x.sample(frac=0.75, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Filter the dataset to include only standard residential sales (PPDCategory_Type 'A')\n",
    "df_sampled = df_sampled[df_sampled['PPDCategory_Type'] == 'A']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values for categorical columns efficiently\n",
    "categorical_columns = ['Property_Type', 'Old/New', 'Duration', 'Postcode', 'Locality', 'Town/City', 'District', 'County', 'PPDCategory_Type']\n",
    "mode_values = {col: df_sampled[col].mode()[0] for col in categorical_columns}\n",
    "df_sampled = df_sampled.fillna(mode_values)\n",
    "\n",
    "# Fill missing 'price' values (numeric) with the median\n",
    "df_sampled['price'] = df_sampled['price'].fillna(df_sampled['price'].median())\n",
    "\n",
    "# Extract date features\n",
    "df_sampled['Date_of_Transfer'] = pd.to_datetime(df_sampled['Date_of_Transfer'])\n",
    "df_sampled['Year'] = df_sampled['Date_of_Transfer'].dt.year\n",
    "df_sampled['Month'] = df_sampled['Date_of_Transfer'].dt.month\n",
    "df_sampled['Day'] = df_sampled['Date_of_Transfer'].dt.day\n",
    "df_sampled['DayOfWeek'] = df_sampled['Date_of_Transfer'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply frequency encoding for categorical variables\n",
    "def optimized_frequency_encoding(df, categorical_columns):\n",
    "    for col in categorical_columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            freq_encoding = df[col].value_counts().to_dict()\n",
    "            df[col + '_freq'] = df[col].map(freq_encoding)\n",
    "            df = df.drop(columns=[col])  # Drop the original object column\n",
    "    return df\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_columns = ['Postcode', 'Property_Type', 'Old/New', 'Duration', 'Locality', 'Town/City', 'District', 'County', 'PPDCategory_Type']\n",
    "\n",
    "# Apply frequency encoding to all categorical columns\n",
    "df_sampled = optimized_frequency_encoding(df_sampled, categorical_columns)\n",
    "\n",
    "# Unified downcast function for all numeric columns\n",
    "def downcast_all_numeric(df):\n",
    "    df[df.select_dtypes(include=['float64']).columns] = df.select_dtypes(include=['float64']).apply(pd.to_numeric, downcast='float')\n",
    "    df[df.select_dtypes(include=['int64']).columns] = df.select_dtypes(include=['int64']).apply(pd.to_numeric, downcast='integer')\n",
    "    return df\n",
    "\n",
    "# Downcast all numeric columns (including frequency-encoded columns)\n",
    "df_sampled = downcast_all_numeric(df_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in X:\n",
      "Year                     int32\n",
      "Month                    int32\n",
      "Day                      int32\n",
      "DayOfWeek                int32\n",
      "Postcode_freq            int16\n",
      "Property_Type_freq       int32\n",
      "Old/New_freq             int32\n",
      "Duration_freq            int32\n",
      "Locality_freq            int32\n",
      "Town/City_freq           int32\n",
      "District_freq            int32\n",
      "County_freq              int32\n",
      "PPDCategory_Type_freq    int32\n",
      "dtype: object\n",
      "All columns are numeric.\n",
      "Shape of X: (20274118, 13)\n",
      "Shape of y: (20274118,)\n"
     ]
    }
   ],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df_sampled.drop(columns=['price', 'Date_of_Transfer'])  # Drop the target 'price' and the raw date column\n",
    "y = df_sampled['price']\n",
    "\n",
    "# Check data types to ensure all columns are numeric\n",
    "print(f\"Data types in X:\\n{X.dtypes}\")\n",
    "if X.select_dtypes(include=['object']).empty:\n",
    "    print(\"All columns are numeric.\")\n",
    "else:\n",
    "    print(\"Some columns are still non-numeric.\")\n",
    "\n",
    "# Print the shape of X and y\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to sample data\n",
    "def sample_data(X, y, sample_size):\n",
    "    if isinstance(sample_size, float):\n",
    "        if 0 < sample_size < 1.0:\n",
    "            return train_test_split(X, y, test_size=0.2, train_size=sample_size, random_state=42)\n",
    "        elif sample_size == 1.0:\n",
    "            return train_test_split(X, y, test_size=0.001, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"sample_size as float must be in the range (0.0, 1.0) or equal to 1.0.\")\n",
    "    elif isinstance(sample_size, int):\n",
    "        if sample_size > len(X):\n",
    "            raise ValueError(f\"sample_size {sample_size} exceeds the number of available samples {len(X)}.\")\n",
    "        sampled_X = X.sample(n=sample_size, random_state=42)\n",
    "        sampled_y = y.loc[sampled_X.index]\n",
    "        return train_test_split(sampled_X, sampled_y, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"sample_size must be a float or an integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Function to calculate and return metrics for Random Forest Regressor\n",
    "def calculate_metrics(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Define hyperparameters for RandomizedSearchCV\n",
    "    param_distributions = {\n",
    "        'n_estimators': [30, 50],  # Moderate number of trees to balance runtime\n",
    "        'max_depth': [5, 10, 20],  # Moderate max depth to prevent overfitting\n",
    "        'min_samples_split': [3, 5, 10],  # Controls complexity\n",
    "        'min_samples_leaf': [2, 3, 5],  # Min samples in leaf node\n",
    "        'max_features': ['sqrt', 'log2'],  # Features to consider at each split\n",
    "        'bootstrap': [True]  # Bootstrapping samples for building trees\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(rf, param_distributions, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_cpu = psutil.cpu_percent(interval=None)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    end_cpu = psutil.cpu_percent(interval=None)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate time and CPU usage\n",
    "    execution_time = end_time - start_time\n",
    "    avg_cpu_usage = (start_cpu + end_cpu) / 2\n",
    "\n",
    "    y_pred = random_search.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate the range of the target variable\n",
    "    target_range = y_train.max() - y_train.min()\n",
    "\n",
    "    # Calculate normalized RMSE (nRMSE)\n",
    "    nrmse = rmse / target_range\n",
    "    \n",
    "    memory_usage_MB = X_train.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    normalized_time = execution_time / memory_usage_MB\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2,\n",
    "        'nRMSE': nrmse,  # Normalized RMSE\n",
    "        'Execution Time (Raw)': execution_time,  # Raw execution time\n",
    "        'Normalized Time (s/MB)': normalized_time,  # Normalized execution time\n",
    "        'Average CPU Usage': avg_cpu_usage\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Function to calculate and return metrics for LightGBM Regressor\n",
    "def calculate_metrics(X_train, X_test, y_train, y_test):\n",
    "    gbm = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "    # Define hyperparameters for RandomizedSearchCV\n",
    "    param_distributions = {\n",
    "        'n_estimators': [30, 50],  # Number of boosting stages\n",
    "        'learning_rate': [0.05, 0.1],  # Step size shrinkage\n",
    "        'max_depth': [3, 5, 7],  # Maximum depth of individual trees\n",
    "        'min_child_samples': [20, 30],  # Minimum number of samples in a leaf\n",
    "        'subsample': [0.8, 0.9],  # Fraction of samples used for training\n",
    "        'colsample_bytree': [0.8, 1.0],  # Fraction of features used per tree\n",
    "        'force_col_wise': [True]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(gbm, param_distributions, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_cpu = psutil.cpu_percent(interval=None)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    end_cpu = psutil.cpu_percent(interval=None)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate time and CPU usage\n",
    "    execution_time = end_time - start_time\n",
    "    avg_cpu_usage = (start_cpu + end_cpu) / 2\n",
    "\n",
    "    y_pred = random_search.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate the range of the target variable\n",
    "    target_range = y_train.max() - y_train.min()\n",
    "\n",
    "    # Calculate normalized RMSE (nRMSE)\n",
    "    nrmse = rmse / target_range\n",
    "    \n",
    "    memory_usage_MB = X_train.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    normalized_time = execution_time / memory_usage_MB\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2,\n",
    "        'nRMSE': nrmse,  # Normalized RMSE\n",
    "        'Execution Time (Raw)': execution_time,  # Raw execution time\n",
    "        'Normalized Time (s/MB)': normalized_time,  # Normalized execution time\n",
    "        'Average CPU Usage': avg_cpu_usage\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 20253843, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 200060.912666\n",
      "Metrics for sample size 1.0:\n",
      "RMSE: 180276.4973174296\n",
      "MAPE: 0.45077831998335915\n",
      "R2: 0.4780971971240777\n",
      "nRMSE: 0.0020030722146722425\n",
      "Execution Time (Raw): 561.3607263565063\n",
      "Normalized Time (s/MB): 0.5010793507187747\n",
      "Average CPU Usage: 55.599999999999994\n",
      "Sample Size: 1.0\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Total Bins 1168\n",
      "[LightGBM] [Info] Number of data points in the train set: 10137059, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 200053.430054\n",
      "Metrics for sample size 0.5:\n",
      "RMSE: 177773.47962789392\n",
      "MAPE: 0.4991461625181108\n",
      "R2: 0.48245470365302334\n",
      "nRMSE: 0.00317452647861465\n",
      "Execution Time (Raw): 251.71198749542236\n",
      "Normalized Time (s/MB): 0.44891471804185223\n",
      "Average CPU Usage: 55.9\n",
      "Sample Size: 0.5\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 5068529, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 200082.113300\n",
      "Metrics for sample size 0.25:\n",
      "RMSE: 178019.51293024333\n",
      "MAPE: 0.5042168495544928\n",
      "R2: 0.4810211772183689\n",
      "nRMSE: 0.003336197831080232\n",
      "Execution Time (Raw): 122.84672927856445\n",
      "Normalized Time (s/MB): 0.4381810403659183\n",
      "Average CPU Usage: 62.2\n",
      "Sample Size: 0.25\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Total Bins 1161\n",
      "[LightGBM] [Info] Number of data points in the train set: 2534264, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 200196.662200\n",
      "Metrics for sample size 0.125:\n",
      "RMSE: 178090.74695983613\n",
      "MAPE: 0.5042675906178975\n",
      "R2: 0.480605758158683\n",
      "nRMSE: 0.003432499435994969\n",
      "Execution Time (Raw): 64.96273374557495\n",
      "Normalized Time (s/MB): 0.4634302279097396\n",
      "Average CPU Usage: 64.9\n",
      "Sample Size: 0.125\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Total Bins 178\n",
      "[LightGBM] [Info] Number of data points in the train set: 80, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 208054.750000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Metrics for sample size 100:\n",
      "RMSE: 96811.26245357978\n",
      "MAPE: 0.4915915059454156\n",
      "R2: 0.43582652606883576\n",
      "nRMSE: 0.10068774046134142\n",
      "Execution Time (Raw): 0.20979785919189453\n",
      "Normalized Time (s/MB): 47.411422413793105\n",
      "Average CPU Usage: 59.599999999999994\n",
      "Sample Size: 100\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 194394.897500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Metrics for sample size 1000:\n",
      "RMSE: 116088.83761468461\n",
      "MAPE: 0.4577951328923384\n",
      "R2: 0.4070083916681999\n",
      "nRMSE: 0.054836484466076814\n",
      "Execution Time (Raw): 0.5389194488525391\n",
      "Normalized Time (s/MB): 12.178836206896552\n",
      "Average CPU Usage: 56.35\n",
      "Sample Size: 1000\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Total Bins 1085\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 203607.979625\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Metrics for sample size 10000:\n",
      "RMSE: 163827.17379930938\n",
      "MAPE: 0.5163396172287066\n",
      "R2: 0.36870680384585686\n",
      "nRMSE: 0.011748094212930039\n",
      "Execution Time (Raw): 0.854790210723877\n",
      "Normalized Time (s/MB): 1.931707974137931\n",
      "Average CPU Usage: 56.6\n",
      "Sample Size: 10000\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Total Bins 1143\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 201164.940362\n",
      "Metrics for sample size 100000:\n",
      "RMSE: 148835.97434717603\n",
      "MAPE: 0.4737839010356831\n",
      "R2: 0.5171678393984653\n",
      "nRMSE: 0.006765379192267878\n",
      "Execution Time (Raw): 2.984917640686035\n",
      "Normalized Time (s/MB): 0.6745502155172414\n",
      "Average CPU Usage: 56.900000000000006\n",
      "Sample Size: 100000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gc # Garbage Collector\n",
    "\n",
    "# Define sample sizes\n",
    "sample_sizes = [1.0, 0.5, 0.25, 0.125, 100, 1000, 10000, 100000]\n",
    "\n",
    "# Initialize metrics storage\n",
    "metrics_list = []\n",
    "total_execution_time = 0\n",
    "total_cpu_usage = 0\n",
    "total_memory_usage_MB = 0\n",
    "\n",
    "# Loop through each sample size\n",
    "for size in sample_sizes:\n",
    "    try:\n",
    "        X_train_sample, X_test_sample, y_train_sample, y_test_sample = sample_data(X, y, size)\n",
    "        metrics = calculate_metrics(X_train_sample, X_test_sample, y_train_sample, y_test_sample)\n",
    "        metrics['Sample Size'] = size\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "        # Call garbage collection after each iteration to free up memory\n",
    "        gc.collect()    \n",
    "\n",
    "        # Accumulate total       \n",
    "        total_execution_time += metrics['Execution Time (Raw)']\n",
    "        total_cpu_usage += metrics['Average CPU Usage']\n",
    "        total_memory_usage_MB += X_train_sample.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "\n",
    "        print(f\"Metrics for sample size {size}:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for sample size {size}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Execution Time for Entire Process (Raw): 16 minutes and 45.47 seconds\n",
      "Total Normalized Execution Time for Entire Process: 0.47732589 seconds per MB\n",
      "Total Average CPU Usage for Entire Process: 58.51%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "      <th>nRMSE</th>\n",
       "      <th>Execution Time (Raw)</th>\n",
       "      <th>Normalized Time (s/MB)</th>\n",
       "      <th>Average CPU Usage</th>\n",
       "      <th>Sample Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180276.497317</td>\n",
       "      <td>0.450778</td>\n",
       "      <td>0.478097</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>561.360726</td>\n",
       "      <td>0.501079</td>\n",
       "      <td>55.60</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>177773.479628</td>\n",
       "      <td>0.499146</td>\n",
       "      <td>0.482455</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>251.711987</td>\n",
       "      <td>0.448915</td>\n",
       "      <td>55.90</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>178019.512930</td>\n",
       "      <td>0.504217</td>\n",
       "      <td>0.481021</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>122.846729</td>\n",
       "      <td>0.438181</td>\n",
       "      <td>62.20</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178090.746960</td>\n",
       "      <td>0.504268</td>\n",
       "      <td>0.480606</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>64.962734</td>\n",
       "      <td>0.463430</td>\n",
       "      <td>64.90</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96811.262454</td>\n",
       "      <td>0.491592</td>\n",
       "      <td>0.435827</td>\n",
       "      <td>0.100688</td>\n",
       "      <td>0.209798</td>\n",
       "      <td>47.411422</td>\n",
       "      <td>59.60</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>116088.837615</td>\n",
       "      <td>0.457795</td>\n",
       "      <td>0.407008</td>\n",
       "      <td>0.054836</td>\n",
       "      <td>0.538919</td>\n",
       "      <td>12.178836</td>\n",
       "      <td>56.35</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>163827.173799</td>\n",
       "      <td>0.516340</td>\n",
       "      <td>0.368707</td>\n",
       "      <td>0.011748</td>\n",
       "      <td>0.854790</td>\n",
       "      <td>1.931708</td>\n",
       "      <td>56.60</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>148835.974347</td>\n",
       "      <td>0.473784</td>\n",
       "      <td>0.517168</td>\n",
       "      <td>0.006765</td>\n",
       "      <td>2.984918</td>\n",
       "      <td>0.674550</td>\n",
       "      <td>56.90</td>\n",
       "      <td>100000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            RMSE      MAPE        R2     nRMSE  Execution Time (Raw)  \\\n",
       "0  180276.497317  0.450778  0.478097  0.002003            561.360726   \n",
       "1  177773.479628  0.499146  0.482455  0.003175            251.711987   \n",
       "2  178019.512930  0.504217  0.481021  0.003336            122.846729   \n",
       "3  178090.746960  0.504268  0.480606  0.003432             64.962734   \n",
       "4   96811.262454  0.491592  0.435827  0.100688              0.209798   \n",
       "5  116088.837615  0.457795  0.407008  0.054836              0.538919   \n",
       "6  163827.173799  0.516340  0.368707  0.011748              0.854790   \n",
       "7  148835.974347  0.473784  0.517168  0.006765              2.984918   \n",
       "\n",
       "   Normalized Time (s/MB)  Average CPU Usage  Sample Size  \n",
       "0                0.501079              55.60        1.000  \n",
       "1                0.448915              55.90        0.500  \n",
       "2                0.438181              62.20        0.250  \n",
       "3                0.463430              64.90        0.125  \n",
       "4               47.411422              59.60      100.000  \n",
       "5               12.178836              56.35     1000.000  \n",
       "6                1.931708              56.60    10000.000  \n",
       "7                0.674550              56.90   100000.000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert metrics to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Calculate total metrics\n",
    "total_avg_cpu_usage = total_cpu_usage / len(sample_sizes)\n",
    "normalized_total_time = total_execution_time / total_memory_usage_MB\n",
    "\n",
    "# Convert total execution time to minutes and seconds\n",
    "total_minutes = int(total_execution_time // 60)\n",
    "total_seconds = total_execution_time % 60\n",
    "\n",
    "# Display total metrics\n",
    "print(f\"Total Execution Time for Entire Process (Raw): {total_minutes} minutes and {total_seconds:.2f} seconds\")\n",
    "print(f\"Total Normalized Execution Time for Entire Process: {normalized_total_time:.8f} seconds per MB\")\n",
    "print(f\"Total Average CPU Usage for Entire Process: {total_avg_cpu_usage:.2f}%\")\n",
    "\n",
    "# Display the metrics DataFrame\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
