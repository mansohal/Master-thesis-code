{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df_sampled \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Guest01/Documents/Manpreet_thesis/Datasets/Glassdoor_job_reviews2/all_reviews.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Downcast numerical columns to save memory\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdowncast\u001b[39m(df):\n",
      "File \u001b[1;32mc:\\Users\\Guest01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Guest01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Guest01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Guest01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1456\u001b[0m, in \u001b[0;36mpandas._libs.parsers._maybe_upcast\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Guest01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\_core\\multiarray.py:1153\u001b[0m, in \u001b[0;36mputmask\u001b[1;34m(a, mask, values)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;124;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \n\u001b[0;32m   1149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mputmask)\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mputmask\u001b[39m(a, \u001b[38;5;241m/\u001b[39m, mask, values):\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03m    putmask(a, mask, values)\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \n\u001b[0;32m   1195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, mask, values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_sampled = pd.read_csv(\"C:/Users/Guest01/Documents/Manpreet_thesis/Datasets/Glassdoor_job_reviews2/all_reviews.csv\")\n",
    "\n",
    "# Downcast numerical columns to save memory\n",
    "def downcast(df):\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols = [c for c in df if df[c].dtype == \"int64\"]\n",
    "    df[float_cols] = df[float_cols].astype(\"float32\")\n",
    "    df[int_cols] = df[int_cols].astype(\"int32\")\n",
    "    return df\n",
    "\n",
    "df_sampled = downcast(df_sampled)\n",
    "df_sampled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pros                         0\n",
       "cons                         0\n",
       "Recommend                    0\n",
       "Career Opportunities         0\n",
       "Compensation and Benefits    0\n",
       "Senior Management            0\n",
       "Work/Life Balance            0\n",
       "Culture & Values             0\n",
       "Diversity & Inclusion        0\n",
       "job                          0\n",
       "status                       0\n",
       "rating                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of relevant features\n",
    "relevant_features = [\n",
    "    'pros', 'cons', 'Recommend', 'Career Opportunities', 'Compensation and Benefits', \n",
    "    'Senior Management', 'Work/Life Balance', 'Culture & Values', 'Diversity & Inclusion', \n",
    "    'job', 'status'\n",
    "]\n",
    "\n",
    "# Fill null values with mode for categorical columns and median for numerical columns\n",
    "def fill_nulls(df, features):\n",
    "    for column in features:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = df[column].fillna(df[column].mode()[0])\n",
    "        else:\n",
    "            df[column] = df[column].fillna(df[column].median())\n",
    "    return df\n",
    "\n",
    "df_sampled = fill_nulls(df_sampled, relevant_features + ['rating'])  # Apply to relevant features and 'rating'\n",
    "\n",
    "# Check for any remaining null values in relevant features\n",
    "df_sampled[relevant_features + ['rating']].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encoding for categorical variables\n",
    "def frequency_encoding(df, columns):\n",
    "    df_copy = df.copy()  # Create a copy of the DataFrame to avoid the warning\n",
    "    for column in columns:\n",
    "        # Generate the frequency encoding (i.e., percentage frequency of each unique value)\n",
    "        freq = df_copy[column].value_counts() / len(df_copy)\n",
    "        # Map the original column with its frequency encoding\n",
    "        df_copy[column] = df_copy[column].map(freq).astype('float32')  # Explicitly cast to float32 to avoid dtype issues\n",
    "    return df_copy\n",
    "\n",
    "# Apply frequency encoding to the relevant non-numeric columns\n",
    "df_sampled = frequency_encoding(df_sampled, relevant_features)\n",
    "\n",
    "# Downcast again to save memory\n",
    "df_sampled = downcast(df_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns in X: Index([], dtype='object')\n",
      "Shape of X: (9901889, 11)\n",
      "Shape of y: (9901889,)\n"
     ]
    }
   ],
   "source": [
    "# Define X and y\n",
    "X = df_sampled[relevant_features]  # Use only relevant features for X\n",
    "y = df_sampled['rating']  # Target variable\n",
    "\n",
    "# Check for non-numeric columns in X\n",
    "non_numeric_columns_in_X = X.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"Non-numeric columns in X:\", non_numeric_columns_in_X)\n",
    "\n",
    "# Print the shapes of X and y\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to sample data\n",
    "def sample_data(X, y, sample_size):\n",
    "    if isinstance(sample_size, float):\n",
    "        if 0 < sample_size < 1.0:\n",
    "            return train_test_split(X, y, test_size=0.2, train_size=sample_size, random_state=42)\n",
    "        elif sample_size == 1.0:\n",
    "            return train_test_split(X, y, test_size=0.2, train_size=None, random_state=42)  # Fix for 1.0 sample size\n",
    "        else:\n",
    "            raise ValueError(\"sample_size as float must be in the range (0.0, 1.0) or equal to 1.0.\")\n",
    "    elif isinstance(sample_size, int):\n",
    "        if sample_size > len(X):\n",
    "            raise ValueError(f\"sample_size {sample_size} exceeds the number of available samples {len(X)}.\")\n",
    "        sampled_X = X.sample(n=sample_size, random_state=42)\n",
    "        sampled_y = y.loc[sampled_X.index]\n",
    "        return train_test_split(sampled_X, sampled_y, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"sample_size must be a float or an integer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, mean_absolute_error, explained_variance_score, max_error\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import psutil\n",
    "\n",
    "# # Function to calculate and return metrics for Gradient Boosting Regressor\n",
    "# def calculate_metrics(X_train, X_test, y_train, y_test):\n",
    "#     gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "#     # Hyperparameter search space\n",
    "#     param_distributions = {\n",
    "#         'n_estimators': [50, 100, 200],  # Number of boosting stages\n",
    "#         'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate shrinks contribution of each tree\n",
    "#         'max_depth': [3, 5, 7, 9],  # Maximum depth of the individual regression estimators\n",
    "#         'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "#         'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required at a leaf node\n",
    "#         'subsample': [0.7, 0.8, 1.0],  # Fraction of samples used for fitting the individual base learners\n",
    "#         'max_features': ['sqrt', 'log2', None],  # Number of features to consider for each split\n",
    "#     }\n",
    "\n",
    "#     random_search = RandomizedSearchCV(gbr, param_distributions, n_iter=20, cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     start_cpu = psutil.cpu_percent(interval=None)\n",
    "#     random_search.fit(X_train, y_train)\n",
    "#     end_cpu = psutil.cpu_percent(interval=None)\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     # Calculate time and CPU usage\n",
    "#     execution_time = end_time - start_time\n",
    "#     avg_cpu_usage = (start_cpu + end_cpu) / 2\n",
    "\n",
    "#     # Make predictions\n",
    "#     y_pred = random_search.predict(X_test)\n",
    "\n",
    "#     # Calculate performance metrics\n",
    "#     rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "#     mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "#     r2 = r2_score(y_test, y_pred)\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     mae = mean_absolute_error(y_test, y_pred)\n",
    "#     max_err = max_error(y_test, y_pred)\n",
    "#     explained_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "#     # Calculate the range of the target variable\n",
    "#     target_range = y_train.max() - y_train.min()\n",
    "\n",
    "#     # Calculate normalized RMSE (nRMSE)\n",
    "#     nrmse = rmse / target_range\n",
    "\n",
    "#     memory_usage_MB = X_train.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "#     normalized_time = execution_time / memory_usage_MB\n",
    "    \n",
    "#     return {\n",
    "#         'RMSE': rmse,\n",
    "#         'nRMSE': nrmse,  # Normalized RMSE\n",
    "#         'MAPE': mape,\n",
    "#         'R2': r2,\n",
    "#         'MSE': mse,\n",
    "#         'MAE': mae,\n",
    "#         'Max Error': max_err,\n",
    "#         'Explained Variance': explained_var,\n",
    "#         'Execution Time (Raw)': execution_time,  # Raw execution time\n",
    "#         'Normalized Time (s/MB)': normalized_time,  # Normalized execution time\n",
    "#         'Average CPU Usage': avg_cpu_usage\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Function to calculate and return metrics for Gradient Boosting Regressor\n",
    "def calculate_metrics(X_train, X_test, y_train, y_test):\n",
    "    gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "    # Define hyperparameters for RandomizedSearchCV\n",
    "    param_distributions = {\n",
    "        'n_estimators': [50, 75],  # Number of boosting stages\n",
    "        'learning_rate': [0.01, 0.05, 0.1],  # Step size shrinkage\n",
    "        'max_depth': [3, 5, 7],  # Maximum depth of individual estimators\n",
    "        'min_samples_split': [5, 10],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 4],  # Minimum number of samples in a leaf node\n",
    "        'subsample': [0.8, 0.9, 1.0],  # Fraction of samples to fit individual base learners\n",
    "        'max_features': ['sqrt', 'log2']  # Features to consider for best split\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(gb, param_distributions, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_cpu = psutil.cpu_percent(interval=None)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    end_cpu = psutil.cpu_percent(interval=None)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate time and CPU usage\n",
    "    execution_time = end_time - start_time\n",
    "    avg_cpu_usage = (start_cpu + end_cpu) / 2\n",
    "\n",
    "    y_pred = random_search.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate the range of the target variable\n",
    "    target_range = y_train.max() - y_train.min()\n",
    "\n",
    "    # Calculate normalized RMSE (nRMSE)\n",
    "    nrmse = rmse / target_range\n",
    "    \n",
    "    memory_usage_MB = X_train.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    normalized_time = execution_time / memory_usage_MB\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2,\n",
    "        'nRMSE': nrmse,  # Normalized RMSE\n",
    "        'Execution Time (Raw)': execution_time,  # Raw execution time\n",
    "        'Normalized Time (s/MB)': normalized_time,  # Normalized execution time\n",
    "        'Average CPU Usage': avg_cpu_usage\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Function to calculate and return metrics for Gradient Boosting Regressor\n",
    "def calculate_metrics(X_train, X_test, y_train, y_test):\n",
    "    gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "    # Optimized hyperparameters for faster execution\n",
    "    # param_distributions = {\n",
    "    #     'n_estimators': [30, 50],  # Reduce number of boosting stages\n",
    "    #     'learning_rate': [0.01, 0.1],  # Higher learning rate to reduce need for trees\n",
    "    #     'max_depth': [5, 10, 20],  # Shallower trees for faster training\n",
    "    #     'min_samples_split': [2, 5, 10],  # Restrict splits to speed up\n",
    "    #     'min_samples_leaf': [1, 2, 4],  # Speed up with slightly higher min samples\n",
    "    #     'subsample': [0.9,1.0],  # No subsampling to avoid extra randomness\n",
    "    #     'max_features': ['sqrt', 'log2']  # Use square root of features to reduce splits\n",
    "    # }\n",
    "\n",
    "    param_distributions = {\n",
    "    'n_estimators': [30, 40],  # Further reduced number of trees\n",
    "    'learning_rate': [0.05, 0.1],  # Balance learning rate for fewer trees\n",
    "    'max_depth': [3, 5],  # Shallower trees for faster training\n",
    "    'min_samples_split': [2, 5],  # More restrictive splits\n",
    "    'min_samples_leaf': [1, 2],  # Fewer samples per leaf\n",
    "    'subsample': [1.0],  # No subsampling for faster processing\n",
    "    'max_features': ['sqrt', 'log2']  # Use sqrt for faster splits\n",
    "}\n",
    "\n",
    "    random_search = RandomizedSearchCV(gb, param_distributions, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_cpu = psutil.cpu_percent(interval=None)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    end_cpu = psutil.cpu_percent(interval=None)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate time and CPU usage\n",
    "    execution_time = end_time - start_time\n",
    "    avg_cpu_usage = (start_cpu + end_cpu) / 2\n",
    "\n",
    "    y_pred = random_search.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate the range of the target variable\n",
    "    target_range = y_train.max() - y_train.min()\n",
    "\n",
    "    # Calculate normalized RMSE (nRMSE)\n",
    "    nrmse = rmse / target_range\n",
    "    \n",
    "    memory_usage_MB = X_train.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    normalized_time = execution_time / memory_usage_MB\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2,\n",
    "        'nRMSE': nrmse,  # Normalized RMSE\n",
    "        'Execution Time (Raw)': execution_time,  # Raw execution time\n",
    "        'Normalized Time (s/MB)': normalized_time,  # Normalized execution time\n",
    "        'Average CPU Usage': avg_cpu_usage\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for sample size 1.0:\n",
      "RMSE: 0.6897360649783185\n",
      "MAPE: 0.1933962448203518\n",
      "R2: 0.6953648646840398\n",
      "nRMSE: 0.17243401624457963\n",
      "Execution Time (Raw): 5020.854091882706\n",
      "Normalized Time (s/MB): 12.781038433610611\n",
      "Average CPU Usage: 48.05\n",
      "Sample Size: 1.0\n",
      "--------------------------------------------------\n",
      "Metrics for sample size 0.5:\n",
      "RMSE: 0.695754906677255\n",
      "MAPE: 0.20287858917686064\n",
      "R2: 0.690024993963559\n",
      "nRMSE: 0.17393872666931376\n",
      "Execution Time (Raw): 3184.7221128940582\n",
      "Normalized Time (s/MB): 12.971198306400682\n",
      "Average CPU Usage: 48.1\n",
      "Sample Size: 0.5\n",
      "--------------------------------------------------\n",
      "Metrics for sample size 0.25:\n",
      "RMSE: 0.7008907869531781\n",
      "MAPE: 0.2045973476889929\n",
      "R2: 0.685431794987204\n",
      "nRMSE: 0.17522269673829452\n",
      "Execution Time (Raw): 1355.220914363861\n",
      "Normalized Time (s/MB): 11.039480749685158\n",
      "Average CPU Usage: 49.55\n",
      "Sample Size: 0.25\n",
      "--------------------------------------------------\n",
      "Metrics for sample size 0.125:\n",
      "RMSE: 0.7061873831334081\n",
      "MAPE: 0.20651296292990953\n",
      "R2: 0.6806594788008056\n",
      "nRMSE: 0.17654684578335203\n",
      "Execution Time (Raw): 576.554619550705\n",
      "Normalized Time (s/MB): 9.393101237165773\n",
      "Average CPU Usage: 50.6\n",
      "Sample Size: 0.125\n",
      "--------------------------------------------------\n",
      "Metrics for sample size 100:\n",
      "RMSE: 0.5142320396633868\n",
      "MAPE: 0.15379768924517206\n",
      "R2: 0.8248777545586974\n",
      "nRMSE: 0.1285580099158467\n",
      "Execution Time (Raw): 0.3500039577484131\n",
      "Normalized Time (s/MB): 88.22253605769231\n",
      "Average CPU Usage: 27.55\n",
      "Sample Size: 100\n",
      "--------------------------------------------------\n",
      "Metrics for sample size 1000:\n",
      "RMSE: 0.7960781676069355\n",
      "MAPE: 0.23838481643656856\n",
      "R2: 0.6124148682402202\n",
      "nRMSE: 0.19901954190173388\n",
      "Execution Time (Raw): 0.4671895503997803\n",
      "Normalized Time (s/MB): 11.776051682692307\n",
      "Average CPU Usage: 33.55\n",
      "Sample Size: 1000\n",
      "--------------------------------------------------\n",
      "Metrics for sample size 10000:\n",
      "RMSE: 0.7414316285017538\n",
      "MAPE: 0.2147046095437364\n",
      "R2: 0.6333110864204261\n",
      "nRMSE: 0.18535790712543845\n",
      "Execution Time (Raw): 1.3024613857269287\n",
      "Normalized Time (s/MB): 3.2830042067307694\n",
      "Average CPU Usage: 47.95\n",
      "Sample Size: 10000\n",
      "--------------------------------------------------\n",
      "Metrics for sample size 100000:\n",
      "RMSE: 0.7226567983517256\n",
      "MAPE: 0.21359196532135052\n",
      "R2: 0.6697374170319839\n",
      "nRMSE: 0.1806641995879314\n",
      "Execution Time (Raw): 14.952486991882324\n",
      "Normalized Time (s/MB): 3.768946875\n",
      "Average CPU Usage: 49.55\n",
      "Sample Size: 100000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gc  # Garbage Collector\n",
    "\n",
    "# Define sample sizes\n",
    "sample_sizes = [1.0, 0.5, 0.25, 0.125, 100, 1000, 10000, 100000]  \n",
    "\n",
    "# Initialize metrics storage\n",
    "metrics_list = []\n",
    "total_execution_time = 0\n",
    "total_cpu_usage = 0\n",
    "total_memory_usage_MB = 0\n",
    "\n",
    "# Loop through each sample size\n",
    "for size in sample_sizes:\n",
    "    try:\n",
    "        # Sample data based on the defined sizes\n",
    "        X_train_sample, X_test_sample, y_train_sample, y_test_sample = sample_data(X, y, size)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(X_train_sample, X_test_sample, y_train_sample, y_test_sample)\n",
    "        metrics['Sample Size'] = size\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "        # Call garbage collection after each iteration to free up memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Accumulate total metrics \n",
    "        total_execution_time += metrics['Execution Time (Raw)']\n",
    "        total_cpu_usage += metrics['Average CPU Usage']\n",
    "        total_memory_usage_MB += X_train_sample.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "\n",
    "        print(f\"Metrics for sample size {size}:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for sample size {size}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Execution Time for Entire Process (Raw): 169 minutes and 14.42 seconds\n",
      "Total Normalized Execution Time for Entire Process: 12.27998731 seconds per MB\n",
      "Total Average CPU Usage for Entire Process: 44.36%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "      <th>nRMSE</th>\n",
       "      <th>Execution Time (Raw)</th>\n",
       "      <th>Normalized Time (s/MB)</th>\n",
       "      <th>Average CPU Usage</th>\n",
       "      <th>Sample Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.689736</td>\n",
       "      <td>0.193396</td>\n",
       "      <td>0.695365</td>\n",
       "      <td>0.172434</td>\n",
       "      <td>5020.854092</td>\n",
       "      <td>12.781038</td>\n",
       "      <td>48.05</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.695755</td>\n",
       "      <td>0.202879</td>\n",
       "      <td>0.690025</td>\n",
       "      <td>0.173939</td>\n",
       "      <td>3184.722113</td>\n",
       "      <td>12.971198</td>\n",
       "      <td>48.10</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.700891</td>\n",
       "      <td>0.204597</td>\n",
       "      <td>0.685432</td>\n",
       "      <td>0.175223</td>\n",
       "      <td>1355.220914</td>\n",
       "      <td>11.039481</td>\n",
       "      <td>49.55</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.706187</td>\n",
       "      <td>0.206513</td>\n",
       "      <td>0.680659</td>\n",
       "      <td>0.176547</td>\n",
       "      <td>576.554620</td>\n",
       "      <td>9.393101</td>\n",
       "      <td>50.60</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.514232</td>\n",
       "      <td>0.153798</td>\n",
       "      <td>0.824878</td>\n",
       "      <td>0.128558</td>\n",
       "      <td>0.350004</td>\n",
       "      <td>88.222536</td>\n",
       "      <td>27.55</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.238385</td>\n",
       "      <td>0.612415</td>\n",
       "      <td>0.199020</td>\n",
       "      <td>0.467190</td>\n",
       "      <td>11.776052</td>\n",
       "      <td>33.55</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.741432</td>\n",
       "      <td>0.214705</td>\n",
       "      <td>0.633311</td>\n",
       "      <td>0.185358</td>\n",
       "      <td>1.302461</td>\n",
       "      <td>3.283004</td>\n",
       "      <td>47.95</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.722657</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.669737</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>14.952487</td>\n",
       "      <td>3.768947</td>\n",
       "      <td>49.55</td>\n",
       "      <td>100000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       RMSE      MAPE        R2     nRMSE  Execution Time (Raw)  \\\n",
       "0  0.689736  0.193396  0.695365  0.172434           5020.854092   \n",
       "1  0.695755  0.202879  0.690025  0.173939           3184.722113   \n",
       "2  0.700891  0.204597  0.685432  0.175223           1355.220914   \n",
       "3  0.706187  0.206513  0.680659  0.176547            576.554620   \n",
       "4  0.514232  0.153798  0.824878  0.128558              0.350004   \n",
       "5  0.796078  0.238385  0.612415  0.199020              0.467190   \n",
       "6  0.741432  0.214705  0.633311  0.185358              1.302461   \n",
       "7  0.722657  0.213592  0.669737  0.180664             14.952487   \n",
       "\n",
       "   Normalized Time (s/MB)  Average CPU Usage  Sample Size  \n",
       "0               12.781038              48.05        1.000  \n",
       "1               12.971198              48.10        0.500  \n",
       "2               11.039481              49.55        0.250  \n",
       "3                9.393101              50.60        0.125  \n",
       "4               88.222536              27.55      100.000  \n",
       "5               11.776052              33.55     1000.000  \n",
       "6                3.283004              47.95    10000.000  \n",
       "7                3.768947              49.55   100000.000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert metrics to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Calculate total metrics\n",
    "total_avg_cpu_usage = total_cpu_usage / len(sample_sizes)\n",
    "normalized_total_time = total_execution_time / total_memory_usage_MB\n",
    "\n",
    "# Convert total execution time to minutes and seconds\n",
    "total_minutes = int(total_execution_time // 60)\n",
    "total_seconds = total_execution_time % 60\n",
    "\n",
    "# Display total metrics\n",
    "print(f\"Total Execution Time for Entire Process (Raw): {total_minutes} minutes and {total_seconds:.2f} seconds\")\n",
    "print(f\"Total Normalized Execution Time for Entire Process: {normalized_total_time:.8f} seconds per MB\")\n",
    "print(f\"Total Average CPU Usage for Entire Process: {total_avg_cpu_usage:.2f}%\")\n",
    "\n",
    "# Display the metrics DataFrame\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another iteraton\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
